# Project: Azure Synapse DSP to Databricks Unity Catalog

## ğŸ“Œ Objective

This project demonstrates a practical migration workflow for moving data from **Azure Synapse Dedicated SQL Pool (DSP)** to **Databricks Unity Catalog**, using Delta Lake's **raw** and **silver** layers. The pipeline includes reading from Synapse, data cleansing, enrichment, and writing to Delta format tables for analytics.

---

## ğŸ“‚ Project Structure

```
AzureSynapse_To_Databricks_Migration/
â”‚
â”œâ”€â”€ data/                           # Dummy 100MB+ realistic data
â”‚   â”œâ”€â”€ sales_transactions.csv
â”‚   â””â”€â”€ customer_data.csv
â”‚
â”œâ”€â”€ notebooks/                      # PySpark scripts
â”‚   â”œâ”€â”€ 01_dsp_read_to_raw.py
â”‚   â”œâ”€â”€ 02_clean_transform_join.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ catalog/                        # SQL schema definition for DSP and for catalog in databricks
â”‚   â””â”€â”€ schema_definition.sql
â”‚
â”œâ”€â”€ architecture.png                # Visual pipeline diagram
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ requirements.txt
```

---

## ğŸ§  Why Migrate to Databricks?

| Feature                  | Synapse DSP       | Databricks Lakehouse    |
|--------------------------|-------------------|--------------------------|
| Compute Flexibility      | Fixed DWU (pre-allocated)         | Auto-scale clusters      |
| Data Format Support      | Tabular only      | Delta, Parquet, CSV, JSON |
| Streaming Support        | Limited           | Built-in Streaming       |
| Cost Efficiency          | High on idle DSP  | Pay-per-use              |
| ML & Data Science        | Separate services | Native to workspace (MLlib, notebooks)     |
| Orchestration            | Basic pipelines   | Complex orchestration + APIs    |
| Catalog Management       | Limited             | Unity Catalog (RBAC, lineage)    |

---

## ğŸ”§ Processing Steps

### Step 1: Read from Synapse DSP â†’ Raw Layer
- Ingest data using JDBC from DSP
- Store in Unity Catalog: `raw.analytics.sales_transaction_raw`

### Step 2: Clean & Transform â†’ Silver Layer
- Normalize strings, handle nulls, correct types
- Join with `customer_data.csv`
- Derive `NetAmount` after discount
- Write to: `main.analytics.sales_transaction_silver`

---

## ğŸ§ª Data Preview / Schema

**Sales Transactions (100k rows)**  
Columns: `TransactionID`, `CustomerID`, `TransactionDate`, `Region`, `ProductCategory`, `Amount`, `PaymentMode`, `DeliveryStatus`, `Rating`, `CustomerFeedback`, ...

**Customer Data (5k rows)**  
Columns: `CustomerID`, `CustomerName`, `Email`, `Country`, `SignupDate`, `Status`, `PreferredChannel`, ...

---

## ğŸ–¼ï¸ Architecture

![Architecture](architecture.png)

---

## ğŸ”Œ Connections & Config

Update `jdbc_url`, `user`, and `password` in:
```python
notebooks/01_dsp_read_to_raw.py
notebooks/utils.py
```

---

## ğŸš€ How to Use

1. Upload `sales_transactions.csv` and `customer_data.csv` to Databricks DBFS or external storage.
2. Update JDBC configs in `01_dsp_read_to_raw.py` and `utils.py`.
3. Run notebooks in sequence.
4. For improved logic, use `02_clean_transform_join.py`.

---

## âœ… Output Tables

| Layer   | Catalog  | Schema    | Table                            |
|---------|----------|-----------|----------------------------------|
| Raw     | raw      | analytics | sales_transaction_raw           |
| Silver  | main     | analytics | sales_transaction_silver        |

---

## ğŸ“ˆ Future Steps

- Add gold layer aggregations
- Integrate with Power BI via Databricks SQL
- Monitor schema changes and automate catalog registration

---
